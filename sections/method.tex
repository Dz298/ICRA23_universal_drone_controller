\section{Learning an Adaptive Drone Controller}
\label{sec:method}

% \todo{Moving the definition intrinsics vector to the introductory paragraph before section A will clarify your high-level exposition before you dive into a more detailed description.}

% \todo{1. Algorithmic novelty As far as I understand, the proposed method is an application of RMA to the quadcopter control. Even though the authors mention that the proposed approach "re-purpose" the adaptation module of RMA to be suitable for the target task, as a reviewer, it was not clear to me the mentioned difference is significant. If the change is substantial, clearly spelling out the difference between the proposed method and RMA enhances the credibility of the manuscript.}

% \todo{- the definition of the 23-dimensional state is not included. }

% \todo{Also, when describing the network architecture, it would be clearer to separate better the description of the factor encoder and the adaptation module; and give more details of the latter for the sake of reproducibility.}

% \todo{- What is the window k used in the adaptation module (it seems the value of k is 400 according to page 4; 0.8 secs).}
We learn a single position controller to fly quadcopters to target positions and stay hovering. This controller takes a history of platform states and commands from a platform-independent high level controller as inputs, and outputs individual motor speed. Our resulting policy can control quadcopters with very different design and hardware characteristics, and is robust to disturbances unseen at training time, such as swing payloads and malfunctioning motors.
%In this work, we view both of these as adaptations to parameters intrinsic and extrinsic to drones, and we will learn a single adaptive policy capable of flying multiple different quadcopter drones under variations in payload and inertia. 
 To achieve this, we use RMA~\cite{kumar2021rma}. However, we re-purpose the adaption module, originally used to estimate parameters external to the robot, e.g. friction, to estimate the robot's intrinsic parameters (e.g. its mass, inertia, motor constant, etc.). In the following, we recapitulate the method of RMA for completeness. Our policy consists of a base policy $\pi$ and an adaptation module $\phi$. At time $t$, the base policy $\pi$ takes the current state $x_t \in \mathbb{R}^{23}$ and an intrinsics vector $z_t \in \mathbb{R}^6$ as input and outputs the target motor speeds $a_t \in \mathbb{R}^4$ for all individual motors. The current state $x_t$ includes the vehicle's position, velocity, rotation matrix, mass-normalized thrust, angular velocity, commanded total thrust and commanded angular velocity. The intrinsics vector $z_t$ is a low dimensional encoding of the environment parameters $e_t$ containing drone parameters, payload, etc. It allows the base policy to adapt to variations in drone parameters, payloads, and disturbances such as external force or torque. Since we cannot directly measure $z_t$ in the real world, we instead estimate it via the adaptation module $\phi$, which uses the discrepancy between the commanded actions and the measured sensor readings from the latest $k$ steps to estimate it online during deployment. More concretely, 
\begin{align}
    \hat{z_t} &=  \phi\big(x_{t-k:t-1}, a_{t-k:t-1}\big) \\
    a_t &= \pi(x_t, \hat{z_t}) \label{eq:pi}
\end{align}

\subsection{Base Policy}
We learn the base policy $\pi$ in simulation using model-free Reinforcement Learning (RL). During training, the base policy takes the current state $x_t$ and the ground-truth intrinsics vector $z_t$ to output control commands $a_t$. We use the environmental factor encoder $\mu$ to compress $e_t$ to $z_t$. This gives us:
\begin{align}
        z_t &= \mu(e_t) \\  
        a_t &= \pi(x_t, z_t)
\end{align}

% \todo{remove previous action from obs}

We implement $\mu$ and $\pi$ as Multi-layer perceptrons (MLP) and jointly train the base policy $\pi$ and the environmental factor encoder $\mu$ end-to-end using model-free RL. RL maximizes the following expected return of the policy $\pi$: 
\begin{align}
    J(\pi) = \mathbb{E}_{\tau \sim p(\tau|\pi)}\Bigg[\sum_{t=0}^{T-1}\gamma^t r_t\Bigg],
\end{align}
where $\tau = \{(x_0, a_0, r_0), (x_1, a_1, r_1) . . .\}$ is the trajectory of the agent when executing the policy $\pi$, and $p(\tau|\pi)$ represents the likelihood of the trajectory under $\pi$.

\paragraph{RL Reward} The following reward function encourages the agent to hover at a goal position and penalizes crashes and oscillating motions. Let us denote the acceleration in the body-fixed thrust axis, i.e. the mass-normalized thrust as $c$, the angular velocity as $\bm{\omega}$, all in the quadcopter's body frame. The commanded mass-normalized thrust is defined as $c_{des}$, commanded angular velocity as $\bm{\omega}_{des}$, all given by the high-level controller.
We additionally define the motor speeds as $\bm{m}$ and the simulation step in the training as $\delta t$. The reward at time $t$ is defined as the sum of the following quantities:
\begin{enumerate}
    \item Angular Velocity Tracking Deviation Penalty: \\$-\| \bm{\omega}^{t} - \bm{\omega}_{des}^{t} \|$
    \item Mass-normalized Thrust Tracking Deviation Penalty: $-\| {c}^{t} - {c}_{des}^{t} \|$
    \item Output Command Oscillation Penalty: $-\| \bm{m}^{t} - \bm{m}^{t-1} \|$
    \item Survival Reward: $\delta t$
\end{enumerate}
The scaling factor of each reward term is $0.01$, $0.02$, $0.0002$, $1$ respectively. 

  
\begin{table}[t]
\caption{\label{tab:randomization} Ranges of the drone and environmental parameters. Parameters without units labelled are dimensionless quantities.}
\setlength{\tabcolsep}{5.5pt}
\begin{center}
\begin{tabular}{lcc}
\toprule
 \textbf{Parameters} & \textbf{Training Range} & \textbf{Testing Range} \\
 \midrule
 Mass (kg) & [0.142, 0.950]  & [0.114, 1.140]  \\
 Arm length (m) & [0.046, 0.200] & [0.037, 0.240] \\
 Mass moment of inertia  & \multirow{2}{*}{ [7.42e-5 , 5.60e-3]}&\multirow{2}{*}{ [5.94e-5 , 6.72e-3]} \\
%  [7.42e-5 , 5.60e-3 ] & [5.94e-5 , 6.72e-3 ]   \\
 around $x$, $y$ (kg$\cdot$m$^2$)&&\\
  Mass moment of inertia  & \multirow{2}{*}{[1.20e-4, 8.80e-3]} &  \multirow{2}{*}{[9.60e-5, 1.06e-2]} \\
  around $z$ (kg$\cdot$m$^2$)&&\\
 Propeller constant $\kappa$ (m) & [0.0041, 0.0168] & [0.0033, 0.0201]\\
 Payload (\% of Mass)  & [10, 50] & [5, 60] \\
 Payload Location & \multirow{3}{*}{[-10, 10]}  & \multirow{3}{*}{[-10, 10]}  \\
 from Center of Mass&&\\
 (\% of Arm length)&&\\
 Motor Constant & [1.15e-7, 7.64e-6] & [9.16e-8, 9.17e-6]  \\
 Body drag coefficient & [0, 0.62]& [0, 0.74] \\
 Max. motor speed (rad/s) & [707, 4895] & [566, 5874] \\
\bottomrule
\end{tabular}
% \marksez{$\kappa$ has units too, if it is "propellerTorqueFromThrust" from our lab sim code, the units are [m]. If it is really `torque to thrust' as in the paper, its units are [1/m].}
\end{center}
\end{table}


% \paragraph{Drone and Environment Randomization} We randomize the drone parameters which includes mass, arm length, inertia matrix, $\kappa$ ( propeller torque to thrust ratio constant), propeller thrust from speed squared constant, body drag coefficient, max motor speed and other external parameters like payload (Table~\ref{tab:randomization}). Note that we define a single vector $e_t$ which includes both the drone and the environment parameters and compress it into a single extrinsics vector $z_t$.  

\subsection{Adaptation Module}
During deployment, we do not have access to the vector $e_t$ and hence we cannot compute the intrinsics vector $z_t$. Instead, we will use the sensor and action history to directly estimate $z_t$ as proposed in \cite{kumar2021rma}. We call this module the adaptation module $\phi$. We can train this module in simulation using supervised learning because we have access to both the ground truth intrinsics $z_t$, and the sensor history and previous actions. We minimize the mean squared error loss $\lVert z - \hat{z} \rVert ^2$.  

\subsection{Deployment}
We directly deploy the base policy $\pi$ which uses the current state $x_t$ and the intrinsics vector $\hat{z_t}$ predicted by the adaptation module $\phi$. We do not calibrate or finetune our policy, and use the same policy without any modifications on the different drones under different payload and conditions.